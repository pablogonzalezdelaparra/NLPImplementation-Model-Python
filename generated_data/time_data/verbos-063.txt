Artificial neural networks had emerged as computationally plausible models of human language processing. A major criticism of these models was that the amount of training data they received far exceeded that of humans during language learning. Here, we used two complementary approaches to ask how the models’ ability to capture human fMRI responses to sentences was affected by the amount of training data. First, we evaluated GPT-2 models trained on 1 million, 10 million, 100 million, or 1 billion words against an fMRI benchmark. We considered the 100-million-word model to be developmentally plausible in terms of the amount of training data given that this amount was similar to what children were estimated to be exposed to during the first 10 years of life. Second, we tested the performance of a GPT-2 model trained on a 9-billion-token dataset to reach state-of-the-art next-word prediction performance on the human benchmark at different stages during training. Across both approaches, we found that (i) the models trained on a developmentally plausible amount of data already achieved near-maximal performance in capturing fMRI responses to sentences. Further, (ii) lower perplexity—a measure of next-word prediction performance—was associated with stronger alignment with human data, suggesting that models that had received enough training to achieve sufficiently high next-word prediction performance also acquired representations of sentences that were predictive of human fMRI responses. In tandem, these findings established that although some training was necessary for the models’ predictive ability, a developmentally realistic amount of training (~100 million words) might suffice.