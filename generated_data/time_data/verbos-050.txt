Implementing empathy to healthcare chatbots is considered promising to create a sense of human warmth. However, existing research frequently overlooked the multidimensionality of empathy, leading to an insufficient understanding of whether artificial empathy is perceived similarly to interpersonal empathy. This paper argued that implementing experiential expressions of empathy may have unintended negative consequences as they might feel inauthentic. Instead, providing instrumental support could be more suitable for modeling artificial empathy as it aligns better with computer-like schemas towards chatbots. Two experimental studies using healthcare chatbots examined the effect of empathetic (feeling with), sympathetic (feeling for), and behavioral-empathetic (empathetic helping) vs. non-empathetic responses on perceived warmth, perceived authenticity, and their consequences on trust and using intentions. Results revealed that any kind of empathy (vs. no empathy) enhanced perceived warmth resulting in higher trust and using intentions. As hypothesized, empathetic and sympathetic responses reduced the chatbot's perceived authenticity, suppressing this positive effect in both studies. A third study did not replicate this backfiring effect in human-human interactions. This research thus highlighted that empathy does not equally apply to human-bot interactions. It further introduced the concept of ‘perceived authenticity’ and demonstrated that distinctively human attributes might backfire by feeling inauthentic in interactions with chatbots.