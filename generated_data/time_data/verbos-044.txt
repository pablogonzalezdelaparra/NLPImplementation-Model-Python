From ELIZA to Alexa, Conversational Agents (CAs) had been deliberately designed to elicit or project empathy. Although empathy could help technology better serve human needs, it could also be deceptive and potentially exploitative. In this work, we characterized empathy in interactions with CAs, highlighting the importance of distinguishing evocations of empathy between two humans from ones between a human and a CA. To this end, we systematically prompted CAs backed by large language models (LLMs) to display empathy while conversing with, or about, 65 distinct human identities, and also compared how different LLMs displayed or modeled empathy. We found that CAs made value judgments about certain identities, and could be encouraging of identities related to harmful ideologies (e.g., Nazism and xenophobia). Moreover, a computational approach to understanding empathy revealed that despite their ability to display empathy, CAs did poorly when interpreting and exploring a userâ€™s experience, contrasting with their human counterparts.