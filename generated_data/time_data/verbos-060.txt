Multimodal Emotion Recognition in Conversations (ERC) aimed to identify the emotions conveyed by each utterance in a conversational video. Current efforts encountered challenges in balancing intra- and inter-speaker context dependencies when tackled intra-modal interactions. This balance was vital as it encompassed modeling self-dependency (emotional inertia) where speakers' own emotions affected them and modeled interpersonal dependencies (empathy) where counterparts' emotions influenced a speaker. Furthermore, challenges arose in addressing cross-modal interactions that involved content with conflicting emotions across different modalities. To address this issue, we introduced an adaptive interactive graph network (IGN) called AdaIGN that employed the Gumbel Softmax trick to adaptively select nodes and edges, enhancing intra- and cross-modal interactions. Unlike undirected graphs, we used a directed IGN to prevent future utterances from impacting the current one. Next, we proposed Node- and Edge-level Selection Policies (NESP) to guide node and edge selection, along with a Graph-Level Selection Policy (GSP) to integrate the utterance representation from original IGN and NESP-enhanced IGN. Moreover, we designed a task-specific loss function that prioritized text modality and intra-speaker context selection. To reduce computational complexity, we used pre-defined pseudo labels through self-supervised methods to mask unnecessary utterance nodes for selection. Experimental results showed that AdaIGN outperformed state-of-the-art methods on two popular datasets. Our code will be available at https://github.com/TuGengs/AdaIGN.