Facial Expression Recognition (FER) was utilized in various fields, such as education, gaming, robotics, healthcare, and others. Facial expression techniques, for instance, an interactive robot with Artificial Intelligence, recognized human faces, detected the emotions of the person it was conversing with, and then used these emotions to choose appropriate responses. One use case for face emotion detection was playing music based on the user’s mood. To do this, we could analyze the user’s facial expression to deduce their feelings. As a result, new emotion models required more investigation as existing ones struggled to correctly measure music’s connection with facial emotion. In this paper, we implemented this kind of job using a Convolution Neural Network (CNN) based deep learning approach. Deep learning could more effectively analyze unstructured data, movies, and other forms of media than machine learning. In our research, we created a real-time system that could recognize human faces, assess human emotions, and even recommend music to users. The OAHEGA and FER-2013 datasets were utilized for experimental study. We created and trained two emotion recognition models using various combinations of these datasets. The proposed model’s accuracy was 73.02%. Using our CNN model, we could predict six emotions: anger, fear, joy, neutral, sadness, and surprise. The proposed system could be utilized in different places where real-time facial recognition played an important role.