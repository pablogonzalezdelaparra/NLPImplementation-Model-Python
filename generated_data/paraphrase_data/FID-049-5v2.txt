Empathy-based healthcare chatbot implementation is seen to be a viable way to instill a feeling of human warmth. But current research often fails to recognize the complexity of empathy, which leaves us with a lack of knowledge if artificial empathy is seen in the same light as interpersonal empathy. This essay makes the case that using experiential forms of empathy might feel inauthentic and have unforeseen harmful effects. Alternatively, as instrumental support is more consistent with computer-like schemas toward chatbots, it could be a better fit for modeling artificial empathy. The effects of sympathetic (feeling with), empathetic (feeling with), and behavioral-empathetic (empathetic helping) vs. non-empathetic responses on perceived warmth, perceived authenticity, and their implications on trust and using intentions are investigated in two experimental studies that use healthcare chatbots. The findings show that any level of empathy, as opposed to none at all, increases perceived warmth, which raises trust and utilizing intentions. Empathic and sympathetic replies, as predicted, lessen the chatbot's perceived authenticity, which in turn suppresses this beneficial impact in both trials. This human-to-human backfiring effect is not replicated in a third investigation. Thus, this study emphasizes that empathy is not universally applicable to interactions between humans and robots. It goes on to explain the idea of "perceived authenticity" and illustrates how having characteristics that are distinctly human might backfire if people feel like they are interacting with chatbots inauthentically.