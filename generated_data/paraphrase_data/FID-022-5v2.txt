With the help of recent technical advancements, computers are now able to recognize and classify facial expressions in order to ascertain an individual's emotional state from a picture or video. The procedure known as "Facial Expression Recognition (FER)" has grown to be one of the most well-liked computer vision research topics. Recent work on deep FER systems has mainly focused on two major issues: overfitting as a result of scarce training data, and expression-unrelated variables such as lighting, head posture, picture quality, and identification bias. This work presents a thorough assessment of deep FER, covering datasets and techniques that provide light on these fundamental issues. First, this work provides an extensive chronology illustrating the development of techniques and datasets for deep facial expression recognition (FER). This timeline shows how the methods and data sources employed in FER have evolved and progressed. After that, a thorough analysis of FER methods is presented, covering everything from the fundamentals of FER (e.g., preprocessing, feature extraction and classification, and methods) to the shift from pro-deep learning (i.e., handcrafted feature-based methods like SVM and HOG) to the deep learning era. Additionally, a brief overview of the benchmark datasets—of which there are two categories: controlled settings (lab) and uncontrolled environments (wild)—that are used to compare and assess various FER models and methodologies is given. We cover current deep neural networks and associated FER-specific training approaches using both static and dynamic picture sequences. Additionally identified are the potential and obstacles that still exist in FER as well as the future paths for creating reliable deep FER systems.