The goal of Multimodal Emotion Recognition in Conversations (ERC) is to recognize the emotions that are expressed by every word in a video discussion. When addressing intra-modal interactions, current approaches struggle to strike a balance between intra- and inter-speaker context dependencies. This delicate balance includes modeling both the self-dependency (emotional inertia) of speakers and the interpersonal dependence (empathy) of speakers' counterparts' emotions on them. In addition, handling cross-modal interactions—which comprise content with contradictory emotions across many modalities—presents difficulties. In order to improve intra- and cross-modal interactions, we provide AdaIGN, an adaptive interactive graph network (IGN) that uses the Gumbel Softmax technique to adaptively pick nodes and edges. We employ a directed IGN, as opposed to undirected graphs, to stop utterances in the future from affecting the present one. Next, we design a task-specific loss function that prioritizes text modality and intra-speaker context selection; we also propose Node- and Edge-level Selection Policies (NESP) to guide node and edge selection, and a Graph-Level Selection Policy (GSP) to integrate the utterance representation from original IGN and NESP-enhanced IGN. We employ pre-defined pseudo labels using self-supervised approaches to mask unneeded utterance nodes for selection in order to minimize computational complexity. On two widely used datasets, experimental findings demonstrate that AdaIGN performs better than state-of-the-art techniques. Our AdaIGN code can be seen at https://github.com/TuGengs/.