Conversational Agents (CAs) have been purposefully created to evoke or portray empathy, from ELIZA to Alexa. While empathy might improve how technology meets human needs, it can also be misleading and even predatory. In this study, we describe empathy in human-CA interactions, emphasizing how crucial it is to discern between human-to-human and human-to-CA evocations of empathy. Towards this goal, we ask CAs supported by large language models (LLMs) to consistently demonstrate empathy when interacting with or discussing 65 different human identities. We also conduct a comparative analysis of the ways in which various LLMs exhibit or represent empathy. We discover that CAs value-judgment certain identities and can promote identities associated with undesirable ideas (such as xenophobia and Nazism). Furthermore, despite their capacity for demonstrating empathy, CAs do worse than human counterparts when it comes to analyzing and delving into a user's experience, according to a computational perspective on empathy.