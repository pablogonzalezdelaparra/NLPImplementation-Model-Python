Many industries, including robotics, gaming, healthcare, and education, use facial expression recognition (FER). With the use of facial expression methods, an artificially intelligent interactive robot can identify human faces, sense the emotions of the person it is speaking with, and utilize those feelings to select the right response. Playing music based on the user's mood is one use of facial expression recognition. We can infer the user's emotions by examining their facial expressions. Because existing emotion models struggle to accurately evaluate the relationship between music and facial expression, further research is needed to develop new ones. In this study, we use a deep learning technique based on Convolution Neural Networks (CNNs) to execute this sort of job. Compared to machine learning, deep learning is more efficient in analyzing unstructured data, movies, and other media. Through our study, we have developed a real-time system that can identify faces, gauge emotions, and even suggest songs to users. The datasets from FER-2013 and OAHEGA were used in the experimental investigation. We employed different combinations of these datasets to develop and train two emotion recognition algorithms. The accuracy of the suggested model is 73.02%. Six emotions may be predicted by our CNN model: neutral, sorrow, surprise, anger, fear, and joy. The suggested technique can be applied in several contexts where instantaneous facial recognition is crucial.