Conversational Agents (CAs), such as ELIZA and Alexa, have been intentionally created to evoke or display empathy. Empathy has the potential to enhance the ability of technology to meet human needs, yet it can also be misleading and sometimes exploitative. This study aims to define and analyze empathy in interactions with conversational agents (CAs). It emphasizes the significance of differentiating between expressions of empathy between two people and those between a human and a CA. In order to achieve this objective, we methodically stimulate conversational agents supported by extensive language models (LLMs) to exhibit empathy towards 65 unique human identities. Additionally, we analyze and contrast the many ways in which different LLMs demonstrate or simulate empathy. Our research reveals that CAs engage in subjective evaluations of certain identities and may promote identities associated with detrimental ideas, such as Nazism and xenophobia. Furthermore, a computational method for comprehending empathy demonstrates that while CAs are capable of exhibiting empathy, they struggle in accurately interpreting and investigating a user's experience, which is in contrast to human beings.