Artificial intelligence (AI) and artificial general intelligence (AGI) have advanced significantly as a result of the search for biologically inspired cognitive architectures, or BICA. But emotions and sentiments are a crucial component of human intelligence that are absent from the majority of BICA models already in use. In this study, an emotion-integrated cognitive architecture that simulates human emotional processing inside a computer framework is developed and put into practice. The Emotion-Integrated Cognitive Architecture (EICA), which we have presented, draws inspiration from the most recent research in affective computing, neurobiology, affective psychology, and neuroscience. In order to create robust, versatile, and adaptive AI agents that can react to complex and dynamic situations with human-like emotional intelligence, EICA seeks to incorporate emotional processing into the heart of the AI system. The EICA model makes use of developments in brain imaging and recording methods to extract information on the neurological underpinnings of human emotions. AI agents are able to detect, understand, and react to emotions in both themselves and other people thanks to the architecture's incorporation of emotion-generating, detection, and control processes. We introduce the notion of EICA, encompassing its modular framework and its interplay with additional cognitive elements. Additionally, we offer case examples that demonstrate how EICA has been successfully used to a range of AI applications, including adaptive robots and virtual assistants. This work advances the computer reproduction of human emotional intelligence, which is a major step towards realizing the BICA Challenge. We are getting closer to reaching the full potential of bi-directional understanding between artificial and biological intelligences by incorporating emotions and feelings into AI systems.