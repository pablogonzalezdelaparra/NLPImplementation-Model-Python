Theory of mind (ToM) in artificial intelligence (AI) has been proposed by some researchers as a result of improvements in the performance of large language models (LLMs). LLMs are more accurate when they are able to impute intents, beliefs, wants, and emotions. Instead of using the distinctly human technique of empathy, they are trained to assign mental states by identifying language patterns in a dataset that does not normally contain that particular person. We inquire as to whether the lack of empathy in LLMs prevents them from respecting a person's right to be an exception, that is, from character evaluations and behavior forecasts that appropriately take into account an individual's uniqueness. Do LLMs have the ability to evaluate a case only on the basis of its similarities to other cases, or can they give due consideration to an individual's assertion that their situation is unique due to interior mental states such as beliefs, wishes, and intentions? We suggest that, in contrast to the importance of predicting accuracy, which is where LLMs shine, the approach of empathy has particular relevance for respecting the right to be an exception. In conclusion, we explore the question of whether applying empathy to analyze unusual instances has more intrinsic or just pragmatic value, and we provide theoretical and empirical directions for furthering this research.