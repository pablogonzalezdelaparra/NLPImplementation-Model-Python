{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1959,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "\"\"\"\n",
    "Uncomment these the first time you run the code\n",
    "\n",
    "# %pip install nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics import pairwise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1960,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "def load_folder(folder):\n",
    "    data = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder, filename), 'r', encoding='utf-8-sig') as f:\n",
    "                data.append(f.read())\n",
    "        break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1961,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        data = file.read()\n",
    "    return [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1962,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data - sentence level\n",
    "def tokenize_data(data):\n",
    "    return [sent_tokenize(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1963,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to lowercase\n",
    "def lower_case(data):\n",
    "    return [[sentence.lower() for sentence in text] for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1964,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-word characters\n",
    "def remove_non_word(data):\n",
    "    return [[re.sub(r'[^\\w\\s]', '', sentence) for sentence in text] for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1965,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def remove_stop_words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [[word for word in text if word not in stop_words] for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1966,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data - word level\n",
    "def tokenize_words(data):\n",
    "    return [word_tokenize(sentence) for text in data for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1967,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the data\n",
    "def lemmatize_data(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [[lemmatizer.lemmatize(word) for word in text] for text in data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1968,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "def flatten_data(data):\n",
    "    return [word for text in data for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1969,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n-grams\n",
    "def get_ngrams(data, n, train_flag=False):\n",
    "    if train_flag:\n",
    "        n_gram = []\n",
    "        for text in data:  \n",
    "            n_gram.append(list(ngrams(text, n)))\n",
    "        return n_gram\n",
    "    \n",
    "    # TODO: Implement case when n = 1\n",
    "    \n",
    "    return list(ngrams(data, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1970,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(test, train):\n",
    "    temp_vector = []\n",
    "    one_hot_test = []\n",
    "\n",
    "    for sentence in train:\n",
    "        for word in test:\n",
    "            if word in sentence:\n",
    "                temp_vector.append(1)\n",
    "            else:\n",
    "                temp_vector.append(0)\n",
    "        one_hot_test.append(temp_vector)\n",
    "        temp_vector = []\n",
    "\n",
    "    return one_hot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1971,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test the model\n",
    "def train_test_model(test):\n",
    "    # Training data\n",
    "    data = load_folder('./train')\n",
    "\n",
    "    sentences_train = tokenize_data(data)\n",
    "    sentences_train = lower_case(sentences_train)\n",
    "    sentences_train = remove_non_word(sentences_train)\n",
    "\n",
    "    sentences_mod_train = tokenize_words(sentences_train)\n",
    "    sentences_mod_train = remove_stop_words(sentences_mod_train)\n",
    "    sentences_mod_train = lemmatize_data(sentences_mod_train)\n",
    "\n",
    "    # Testing data\n",
    "    sentences_test = tokenize_data(test)\n",
    "    sentences_test = lower_case(sentences_test)\n",
    "    sentences_test = remove_non_word(sentences_test)\n",
    "\n",
    "    sentences_mod_test = tokenize_words(sentences_test)\n",
    "    sentences_mod_test = remove_stop_words(sentences_mod_test)\n",
    "    sentences_mod_test = lemmatize_data(sentences_mod_test)\n",
    "\n",
    "    # Data conversion\n",
    "    sentences_mod_test = flatten_data(sentences_mod_test)\n",
    "    sentences_mod_train = get_ngrams(sentences_mod_train, 2, True)\n",
    "    sentences_mod_test = get_ngrams(sentences_mod_test, 2)\n",
    "\n",
    "    # One hot encoding\n",
    "    one_hot_test = one_hot_encoding(sentences_mod_test, sentences_mod_train)\n",
    "\n",
    "    return one_hot_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1972,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './test_dummy/FID-01.txt'\n",
    "test = load_file(file_path)\n",
    "\n",
    "mat = train_test_model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1973,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mat)):\n",
    "    print(mat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1974,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.07744031, 0.22750788, 0.        ,\n",
       "        0.12768848],\n",
       "       [0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.07744031, 0.        , 1.        , 0.08104409, 0.        ,\n",
       "        0.        ],\n",
       "       [0.22750788, 0.        , 0.08104409, 1.        , 0.        ,\n",
       "        0.13363062],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.12768848, 0.        , 0.        , 0.13363062, 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 1974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise.cosine_similarity(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1975,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.array(pairwise.cosine_similarity(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1976,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of Maximum Similarity Scores: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum similarity score from each row\n",
    "max_similarity_scores = np.mean(similarity_matrix, axis=1)\n",
    "\n",
    "# Calculate the average of the maximum similarity scores\n",
    "average_max_similarity = np.max(max_similarity_scores)\n",
    "\n",
    "print(\"Average of Maximum Similarity Scores:\", round(average_max_similarity, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
