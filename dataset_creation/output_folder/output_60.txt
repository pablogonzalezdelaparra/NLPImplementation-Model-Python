The objective of Multimodal Emotion Recognition in Conversations (ERC) is to detect and classify the emotions expressed in each phrase inside a video of a discussion. Present endeavors face difficulties in achieving a balance between the contextual dependencies inside and across speakers while addressing interactions within the same mode. The balance described here is crucial because it includes both the modeling of self-dependency, where a speaker's own emotions impact them, and the modeling of interpersonal dependencies, where a speaker is influenced by the emotions of others. Moreover, there are difficulties in dealing with cross-modal interactions that entail content with contradictory emotions across several modalities. In order to tackle this problem, we propose the implementation of an adaptive interactive graph network (IGN) known as AdaIGN. This network utilizes the Gumbel Softmax technique to dynamically choose nodes and edges, hence improving interactions inside and across different modes. In contrast to undirected graphs, we employ a directed IGN (Interactive Graph Network) to ensure that future utterances do not influence the current one. Our proposal involves the implementation of Node- and Edge-level Selection Policies (NESP) to provide guidance for selecting nodes and edges. Additionally, we introduce a Graph-Level Selection Policy (GSP) to combine the utterance representation from the original IGN and the NESP-enhanced IGN. Furthermore, we have developed a task-specific loss function that gives priority to text modality and intra-speaker context selection. In order to decrease the computational complexity, we employ pre-determined pseudo labels generated by self-supervised techniques to conceal irrelevant utterance nodes for selection. Empirical evidence demonstrates that AdaIGN surpasses state-of-the-art techniques on two widely used datasets. The code will be accessible on the GitHub repository at https://github.com/TuGengs/AdaIGN.