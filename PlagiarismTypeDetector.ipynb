{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BKPsmxNkfrJQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/pablogdlp/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import keras\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda, Dense\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "79dXdeZXNmcI"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv('data.csv')\n",
        "data = data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plagiarized        0\n",
            "original           0\n",
            "plagiarism_type    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plagiarized</th>\n",
              "      <th>original</th>\n",
              "      <th>plagiarism_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It compares these algorithms with the non-fuse...</td>\n",
              "      <td>Given the challenges of inter-domain informati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This paper utilizes a fuzzy logic controller (...</td>\n",
              "      <td>﻿This article employs a fuzzy logic controller...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drug designing and development is an important...</td>\n",
              "      <td>Drug designing and development is an important...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Human-AI interaction has become an important f...</td>\n",
              "      <td>Human-AI interaction has become an important f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From ELIZA to Alexa, Conversational Agents (CA...</td>\n",
              "      <td>From ELIZA to Alexa, Conversational Agents (CA...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>This paper looks at a way to improve the dynam...</td>\n",
              "      <td>﻿This article examines a solution to the major...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>This article explores a solution to address th...</td>\n",
              "      <td>﻿This article examines a solution to the major...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>A lot of industries, including education, are ...</td>\n",
              "      <td>Artificial intelligence (AI) is rapidly transf...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>Network news is an important way for netizens ...</td>\n",
              "      <td>Network news is an important way for netizens ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>This article examines the factors that contrib...</td>\n",
              "      <td>﻿In this article, the causes of technological ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>660 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           plagiarized  \\\n",
              "0    It compares these algorithms with the non-fuse...   \n",
              "1    This paper utilizes a fuzzy logic controller (...   \n",
              "2    Drug designing and development is an important...   \n",
              "3    Human-AI interaction has become an important f...   \n",
              "4    From ELIZA to Alexa, Conversational Agents (CA...   \n",
              "..                                                 ...   \n",
              "655  This paper looks at a way to improve the dynam...   \n",
              "656  This article explores a solution to address th...   \n",
              "657  A lot of industries, including education, are ...   \n",
              "658  Network news is an important way for netizens ...   \n",
              "659  This article examines the factors that contrib...   \n",
              "\n",
              "                                              original  plagiarism_type  \n",
              "0    Given the challenges of inter-domain informati...                1  \n",
              "1    ﻿This article employs a fuzzy logic controller...                2  \n",
              "2    Drug designing and development is an important...                1  \n",
              "3    Human-AI interaction has become an important f...                0  \n",
              "4    From ELIZA to Alexa, Conversational Agents (CA...                1  \n",
              "..                                                 ...              ...  \n",
              "655  ﻿This article examines a solution to the major...                2  \n",
              "656  ﻿This article examines a solution to the major...                2  \n",
              "657  Artificial intelligence (AI) is rapidly transf...                2  \n",
              "658  Network news is an important way for netizens ...                1  \n",
              "659  ﻿In this article, the causes of technological ...                2  \n",
              "\n",
              "[660 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocess data\n",
        "print(data.isna().sum())\n",
        "data.dropna(inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XBC961bCOeMc"
      },
      "outputs": [],
      "source": [
        "# Assign data to variables\n",
        "plagiarized_texts = data['plagiarized']\n",
        "original_texts = data['original']\n",
        "labels = data['plagiarism_type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "llfmq_g8OlzI"
      },
      "outputs": [],
      "source": [
        "# Fill missing values\n",
        "plagiarized_texts = plagiarized_texts.fillna('')\n",
        "original_texts = original_texts.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VBrZiejmOsHJ"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "max_sequence_length = 800\n",
        "embedding_dim = 300\n",
        "num_classes = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'It compares these algorithms with the non-fused approach. The study begins by collecting Douban movie rating data and social network information. To ensure data integrity, Levenshtein distance detection is employed to remove duplicate scores, while natural language processing technology is utilized to extract keywords and topic information from social texts. Additionally, graph convolutional networks are utilized to convert user relationships into feature vectors, and a unique thermal coding method is used to convert discrete user and movie information into binary matrices. To prevent overfitting, the Ridge regularization method is introduced to gradually optimize potential feature vectors. Weighted average and feature connection techniques are then applied to integrate features from different fields. In the experimental stage, the paper conducts cross-domain information fusion optimization on four mainstream mathematical matrix decomposition algorithms: alternating least squares method, non-negative matrix decomposition, singular value decomposition, and latent factor model (LFM). Moreover, the paper combines the item-based collaborative filtering algorithm with merged user characteristics to generate personalized recommendation lists. The results indicate a significant improvement in score accuracy, with mean absolute error and root mean squared error reduced by 12.8% and 13.2% respectively across the four algorithms. Overall, the mathematical matrix decomposition algorithm combined with cross-domain information fusion demonstrates clear advantages in accuracy, prediction performance, recommendation diversity, and ranking quality, and improves the accuracy and diversity of the recommendation system. Additionally, when k\\u2009=\\u200910, the average F1 score reaches 0.97, and the ranking accuracy coverage of the LFM algorithm increases by 54.2%. Given the challenges of inter-domain information fusion and data sparsity in collaborative filtering algorithms, this paper proposes a cross-domain information fusion matrix decomposition algorithm to enhance the accuracy of personalized recommendations in artificial intelligence recommendation systems. By effectively addressing collaborative filtering challenges through the integration of diverse techniques, it significantly surpasses traditional models in recommendation accuracy and variety. Given the challenges of inter-domain information fusion and data sparsity in collaborative filtering algorithms, this paper proposes a cross-domain information fusion matrix decomposition algorithm to enhance the accuracy of personalized recommendations in artificial intelligence recommendation systems. The study begins by collecting Douban movie rating data and social network information. To ensure data integrity, Levenshtein distance detection is employed to remove duplicate scores, while natural language processing technology is utilized to extract keywords and topic information from social texts. Additionally, graph convolutional networks are utilized to convert user relationships into feature vectors, and a unique thermal coding method is used to convert discrete user and movie information into binary matrices. To prevent overfitting, the Ridge regularization method is introduced to gradually optimize potential feature vectors. Weighted average and feature connection techniques are then applied to integrate features from different fields. Moreover, the paper combines the item-based collaborative filtering algorithm with merged user characteristics to generate personalized recommendation lists. In the experimental stage, the paper conducts cross-domain information fusion optimization on four mainstream mathematical matrix decomposition algorithms: alternating least squares method, non-negative matrix decomposition, singular value decomposition, and latent factor model (LFM). It compares these algorithms with the non-fused approach. The results indicate a significant improvement in score accuracy, with mean absolute error and root mean squared error reduced by 12.8% and 13.2% respectively across the four algorithms. Additionally, when k\\u2009=\\u200910, the average F1 score reaches 0.97, and the ranking accuracy coverage of the LFM algorithm increases by 54.2%. Overall, the mathematical matrix decomposition algorithm combined with cross-domain information fusion demonstrates clear advantages in accuracy, prediction performance, recommendation diversity, and ranking quality, and improves the accuracy and diversity of the recommendation system. By effectively addressing collaborative filtering challenges through the integration of diverse techniques, it significantly surpasses traditional models in recommendation accuracy and variety.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Combine texts\n",
        "texts = (plagiarized_texts + ' ' + original_texts).astype(str)\n",
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gSpYva4lO5Jg"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts\n",
        "tokens = [word for sentence in texts for word in sentence.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUPwWpLDPFUb",
        "outputId": "71ea4df6-6b79-4d30-f6bf-c94f8de720ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8405"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get vocabulary size\n",
        "vocabulary_size = len(set(tokens))\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Jg7Z1C4WPSCX"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bdbIjHNvRjXM"
      },
      "outputs": [],
      "source": [
        "# Convert texts to sequences and pad them\n",
        "sequences_plagiarized = tokenizer.texts_to_sequences(plagiarized_texts)\n",
        "sequences_original = tokenizer.texts_to_sequences(original_texts)\n",
        "padded_sequences_plagiarized = pad_sequences(sequences_plagiarized, maxlen=max_sequence_length)\n",
        "padded_sequences_original = pad_sequences(sequences_original, maxlen=max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW56IBpRUhDM",
        "outputId": "f2ab36ca-b055-4f0d-933a-7697cc35a12d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ninput_layer1 = Input(shape=(max_sequence_length,))\\ninput_layer2 = Input(shape=(max_sequence_length,))\\n\\nembedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\\n\\nlstm_layer = LSTM(units=50)\\n\\nx1 = embedding_layer(input_layer1)\\nx1 = lstm_layer(x1)\\n\\nx2 = embedding_layer(input_layer2)\\nx2 = lstm_layer(x2)\\n\\ndistance_layer = Lambda(lambda x: tf.keras.backend.abs(x[0] - x[1]),\\n                        output_shape=lambda _: (1,))([x1, x2])\\n\\noutput_layer = Dense(num_classes, activation='softmax')(distance_layer)\\n\\nmodel = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\\n\\nmodel.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\n\\ncallbacks = [\\n    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\\n]\\n\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define model (First version)\n",
        "\"\"\"\n",
        "input_layer1 = Input(shape=(max_sequence_length,))\n",
        "input_layer2 = Input(shape=(max_sequence_length,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
        "\n",
        "lstm_layer = LSTM(units=50)\n",
        "\n",
        "x1 = embedding_layer(input_layer1)\n",
        "x1 = lstm_layer(x1)\n",
        "\n",
        "x2 = embedding_layer(input_layer2)\n",
        "x2 = lstm_layer(x2)\n",
        "\n",
        "distance_layer = Lambda(lambda x: tf.keras.backend.abs(x[0] - x[1]),\n",
        "                        output_shape=lambda _: (1,))([x1, x2])\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(distance_layer)\n",
        "\n",
        "model = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n# Define model (Second version)\\nfrom keras.layers import LSTM, Embedding, Dense, Input, Concatenate, Dropout\\n\\n# Define model\\ninput_layer1 = Input(shape=(max_sequence_length,))\\ninput_layer2 = Input(shape=(max_sequence_length,))\\n\\nembedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\\n\\n# First LSTM layer\\nlstm_layer1 = LSTM(units=100, return_sequences=True)\\nx1 = embedding_layer(input_layer1)\\nx1 = lstm_layer1(x1)\\n\\n# Second LSTM layer\\nlstm_layer2 = LSTM(units=50)\\nx1 = lstm_layer2(x1)\\n\\n# Repeat for the second input\\nx2 = embedding_layer(input_layer2)\\nx2 = lstm_layer1(x2)\\nx2 = lstm_layer2(x2)\\n\\n# Concatenate the LSTM outputs\\nconcatenated = Concatenate()([x1, x2])\\n\\n# Add additional layers for processing\\nx = Dense(128, activation='relu')(concatenated)\\nx = Dropout(0.5)(x)\\n\\noutput_layer = Dense(num_classes, activation='softmax')(x)\\n\\nmodel = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\\n\\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\n\\ncallbacks = [\\n    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\\n]\\n\\nmodel.summary()\\n\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Define model (Second version)\n",
        "from keras.layers import LSTM, Embedding, Dense, Input, Concatenate, Dropout\n",
        "\n",
        "# Define model\n",
        "input_layer1 = Input(shape=(max_sequence_length,))\n",
        "input_layer2 = Input(shape=(max_sequence_length,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
        "\n",
        "# First LSTM layer\n",
        "lstm_layer1 = LSTM(units=100, return_sequences=True)\n",
        "x1 = embedding_layer(input_layer1)\n",
        "x1 = lstm_layer1(x1)\n",
        "\n",
        "# Second LSTM layer\n",
        "lstm_layer2 = LSTM(units=50)\n",
        "x1 = lstm_layer2(x1)\n",
        "\n",
        "# Repeat for the second input\n",
        "x2 = embedding_layer(input_layer2)\n",
        "x2 = lstm_layer1(x2)\n",
        "x2 = lstm_layer2(x2)\n",
        "\n",
        "# Concatenate the LSTM outputs\n",
        "concatenated = Concatenate()([x1, x2])\n",
        "\n",
        "# Add additional layers for processing\n",
        "x = Dense(128, activation='relu')(concatenated)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "]\n",
        "\n",
        "model.summary()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "#TODO: CHECK\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Layer, MultiHeadAttention, Dense, Dropout, LayerNormalization\n",
        "\n",
        "class TransformerLayer(Layer):\n",
        "    def __init__(self, num_heads, feed_forward_dim, dropout_rate=0.1, **kwargs):\n",
        "        super(TransformerLayer, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.feed_forward_dim = feed_forward_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=input_shape[-1])\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(self.feed_forward_dim, activation='relu'),\n",
        "            Dense(input_shape[-1])\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(self.dropout_rate)\n",
        "        self.dropout2 = Dropout(self.dropout_rate)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Adjusted model with improved strategies\n",
        "input_layer1 = Input(shape=(max_sequence_length,))\n",
        "input_layer2 = Input(shape=(max_sequence_length,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
        "\n",
        "# Apply embedding\n",
        "x1 = embedding_layer(input_layer1)\n",
        "x2 = embedding_layer(input_layer2)\n",
        "\n",
        "# Transformer layers\n",
        "transformer_layer1 = TransformerLayer(num_heads=2, feed_forward_dim=64, dropout_rate=0.2)\n",
        "x1 = transformer_layer1(x1)\n",
        "\n",
        "transformer_layer2 = TransformerLayer(num_heads=2, feed_forward_dim=64, dropout_rate=0.2)\n",
        "x2 = transformer_layer2(x2)\n",
        "\n",
        "# Pooling layer\n",
        "x1 = GlobalAveragePooling1D()(x1)\n",
        "x2 = GlobalAveragePooling1D()(x2)\n",
        "\n",
        "# Concatenate the Transformer outputs\n",
        "concatenated = Concatenate()([x1, x2])\n",
        "\n",
        "# Add additional layers for processing\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(concatenated)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\n",
        "\n",
        "# Adjusted optimizer\n",
        "optimizer = Adam(lr=0.0001)  # Lowered learning rate\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii6hTkO0cFBF",
        "outputId": "71c07100-ac7a-4091-bb54-b7d5ab8e7ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)       [(None, 800)]                0         []                            \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)       [(None, 800)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)     (None, 800, 300)             2521500   ['input_13[0][0]',            \n",
            "                                                                     'input_14[0][0]']            \n",
            "                                                                                                  \n",
            " transformer_layer_7 (Trans  (None, 800, 300)             762064    ['embedding_6[0][0]']         \n",
            " formerLayer)                                                                                     \n",
            "                                                                                                  \n",
            " transformer_layer_8 (Trans  (None, 800, 300)             762064    ['embedding_6[1][0]']         \n",
            " formerLayer)                                                                                     \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 300)                  0         ['transformer_layer_7[0][0]'] \n",
            " 2 (GlobalAveragePooling1D)                                                                       \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 300)                  0         ['transformer_layer_8[0][0]'] \n",
            " 3 (GlobalAveragePooling1D)                                                                       \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate  (None, 600)                  0         ['global_average_pooling1d_12[\n",
            " )                                                                  0][0]',                       \n",
            "                                                                     'global_average_pooling1d_13[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 128)                  76928     ['concatenate_6[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 128)                  0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 3)                    387       ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4122943 (15.73 MB)\n",
            "Trainable params: 4122943 (15.73 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Show the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "17/17 [==============================] - 47s 3s/step - loss: 1.7258 - accuracy: 0.3030 - val_loss: 1.3477 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "17/17 [==============================] - 50s 3s/step - loss: 1.2979 - accuracy: 0.3087 - val_loss: 1.2856 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "17/17 [==============================] - 57s 3s/step - loss: 1.2684 - accuracy: 0.3504 - val_loss: 1.2611 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "17/17 [==============================] - 54s 3s/step - loss: 1.2542 - accuracy: 0.3788 - val_loss: 1.2678 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "17/17 [==============================] - 52s 3s/step - loss: 1.2439 - accuracy: 0.3447 - val_loss: 1.2476 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "17/17 [==============================] - 51s 3s/step - loss: 1.2429 - accuracy: 0.3561 - val_loss: 1.2436 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "17/17 [==============================] - 53s 3s/step - loss: 1.2290 - accuracy: 0.3598 - val_loss: 1.2319 - val_accuracy: 0.3636 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "17/17 [==============================] - 52s 3s/step - loss: 1.2260 - accuracy: 0.3409 - val_loss: 1.2221 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "17/17 [==============================] - 55s 3s/step - loss: 1.2205 - accuracy: 0.3504 - val_loss: 1.2176 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "17/17 [==============================] - 54s 3s/step - loss: 1.2132 - accuracy: 0.3598 - val_loss: 1.2138 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "17/17 [==============================] - 53s 3s/step - loss: 1.2086 - accuracy: 0.3598 - val_loss: 1.2100 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "17/17 [==============================] - 53s 3s/step - loss: 1.2042 - accuracy: 0.3598 - val_loss: 1.2065 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "13/17 [=====================>........] - ETA: 12s - loss: 1.1976 - accuracy: 0.3702"
          ]
        }
      ],
      "source": [
        "# Run the model\n",
        "model.fit([padded_sequences_plagiarized, padded_sequences_original], labels, epochs=20,\n",
        "          batch_size=32, validation_split=0.2, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESjzjNQ_WXEO"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Preprocess function\n",
        "def preprocess(sentence):\n",
        "  sequence = pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen=max_sequence_length)\n",
        "  return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QhKEarqWGgB",
        "outputId": "9c58f4f6-1c8e-484f-f4bb-ec6ef9a48499"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# FID-01.txt and org-076.txt -> 0\n",
        "prediction = model.predict([\n",
        "    preprocess(\"This article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints. Traditional cheating detection methods have many disadvantages, such as difficult to detect covert equipment cheating, multi-source cheating, difficult to distinguish plagiarists from plagiarists, difficult to distinguish plagiarists from victims, or plagiarism from coincidences. To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics. In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields. In this paper, the concept of knowledge point mastery Index is introduced to measure students’ mastery of a certain knowledge point, and a test method of cheating based on improved cognitive diagnostic model is proposed. Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader. We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB). Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems. The experiments show that the precision and recall rate of this method are significantly higher than those of the method based on the false-same rate, the method based on the false-same rate and the right-same rate and the method based on the Person-Fit index.\"), \n",
        "    preprocess(\"This article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints. To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics. In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields. In this regard, we introduce a general barrier function and prescribed performance methodology to tackle constrained range impediments effectively. Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader. With the aid of the dynamic surface technology, we propose a decentralized adaptive event-triggering fault-tolerant (ETFT) formation control strategy. We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB). Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems.\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# FID-03.txt and Org-016.txt -> 1\n",
        "prediction = model.predict([\n",
        "    preprocess(\"At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response. However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc. At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance. The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively. This paper designed an EP Vision System (VS) based on AI technology. The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions. It showed the positive relationship between AI technology and EP VS. The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. \"), \n",
        "    preprocess(\"At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response. However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc. At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance. This paper designed an EP Vision System (VS) based on AI technology. The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions. The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent. It showed the positive relationship between AI technology and EP VS.\")])\n",
        "predicted_classes = np.argmax(prediction, axis=1)\n",
        "predicted_classes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# FID-09.txt and Org-109.txt -> 2\n",
        "prediction = model.predict([\n",
        "    preprocess(\"Drug designing and development represent crucial areas of research for pharmaceutical companies and chemical scientists. However, challenges such as low efficacy, off-target delivery, time consumption, and high cost hinder progress in drug design and discovery. Additionally, the complexity and volume of data from genomics, proteomics, microarray data, and clinical trials pose significant obstacles in the drug discovery pipeline. Artificial intelligence (AI) and machine learning (ML) technologies have revolutionized drug discovery and development, particularly through the use of artificial neural networks and deep learning algorithms. These technologies have modernized various processes in drug discovery, including peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Historical evidence supports the implementation of AI and deep learning in drug discovery. Furthermore, novel data mining, curation, and management techniques have provided critical support to newly developed modeling algorithms. In summary, advancements in AI and deep learning offer significant opportunities for rational drug design and discovery, ultimately benefiting mankind. Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. \"), \n",
        "    preprocess(\"Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. \")])\n",
        "predicted_classes = np.argmax(prediction, axis=1)\n",
        "predicted_classes[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
