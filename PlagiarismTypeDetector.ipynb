{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BKPsmxNkfrJQ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import keras\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda, Dense\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "79dXdeZXNmcI"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv('data.csv')\n",
        "data = data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plagiarized        0\n",
            "original           0\n",
            "plagiarism_type    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plagiarized</th>\n",
              "      <th>original</th>\n",
              "      <th>plagiarism_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Integrating empathy into healthcare chatbots i...</td>\n",
              "      <td>Implementing empathy to healthcare chatbots is...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It is still debatable if using exergames in ph...</td>\n",
              "      <td>﻿Whether the application of exergames in physi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Inverse gas chromatography (IGC) is a method t...</td>\n",
              "      <td>Inverse gas chromatography (IGC) has emerged a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Network news serves as a crucial means for net...</td>\n",
              "      <td>Network news is an important way for netizens ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Current efforts encounter challenges in balanc...</td>\n",
              "      <td>Multimodal Emotion Recognition in Conversation...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>Artificial Intelligence (AI) is reshaping the ...</td>\n",
              "      <td>Artificial Intelligence (AI) is reshaping the ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>The application of AI and machine learning, pa...</td>\n",
              "      <td>The application of AI and machine learning, pa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>Utilizing AI and machine learning, namely the ...</td>\n",
              "      <td>The application of AI and machine learning, pa...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>Accurately scientific disciplines, including b...</td>\n",
              "      <td>Accurately scientific disciplines, including b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>The complexity of adaptive fuzzy event-trigger...</td>\n",
              "      <td>﻿This article delves into the intricacies of a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>660 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           plagiarized  \\\n",
              "0    Integrating empathy into healthcare chatbots i...   \n",
              "1    It is still debatable if using exergames in ph...   \n",
              "2    Inverse gas chromatography (IGC) is a method t...   \n",
              "3    Network news serves as a crucial means for net...   \n",
              "4    Current efforts encounter challenges in balanc...   \n",
              "..                                                 ...   \n",
              "655  Artificial Intelligence (AI) is reshaping the ...   \n",
              "656  The application of AI and machine learning, pa...   \n",
              "657  Utilizing AI and machine learning, namely the ...   \n",
              "658  Accurately scientific disciplines, including b...   \n",
              "659  The complexity of adaptive fuzzy event-trigger...   \n",
              "\n",
              "                                              original  plagiarism_type  \n",
              "0    Implementing empathy to healthcare chatbots is...                2  \n",
              "1    ﻿Whether the application of exergames in physi...                2  \n",
              "2    Inverse gas chromatography (IGC) has emerged a...                2  \n",
              "3    Network news is an important way for netizens ...                2  \n",
              "4    Multimodal Emotion Recognition in Conversation...                1  \n",
              "..                                                 ...              ...  \n",
              "655  Artificial Intelligence (AI) is reshaping the ...                1  \n",
              "656  The application of AI and machine learning, pa...                0  \n",
              "657  The application of AI and machine learning, pa...                2  \n",
              "658  Accurately scientific disciplines, including b...                1  \n",
              "659  ﻿This article delves into the intricacies of a...                2  \n",
              "\n",
              "[660 rows x 3 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocess data\n",
        "print(data.isna().sum())\n",
        "data.dropna(inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XBC961bCOeMc"
      },
      "outputs": [],
      "source": [
        "# Assign data to variables\n",
        "plagiarized_texts = data['plagiarized']\n",
        "original_texts = data['original']\n",
        "labels = data['plagiarism_type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "llfmq_g8OlzI"
      },
      "outputs": [],
      "source": [
        "# Fill missing values\n",
        "plagiarized_texts = plagiarized_texts.fillna('')\n",
        "original_texts = original_texts.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VBrZiejmOsHJ"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "max_sequence_length = 800\n",
        "embedding_dim = 300\n",
        "num_classes = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Integrating empathy into healthcare chatbots is seen as a viable approach to evoke a feeling of human warmth. Nevertheless, current research often fails to consider the multifaceted nature of empathy, resulting in a limited comprehension of whether manufactured empathy is experienced in a similar manner to interpersonal empathy. This research contends that the implementation of experiential manifestations of empathy may result in unforeseen adverse effects due to their potential inauthenticity. Alternatively, offering instrumental assistance may be more appropriate for simulating artificial empathy, as it is more compatible with computer-based frameworks used in chatbots. Two empirical investigations utilizing healthcare chatbots investigate the impact of empathetic (experiencing with), sympathetic (experiencing for), and behavioral-empathetic (empathetic aiding) versus non-empathetic responses on the perception of warmth, perception of authenticity, and their subsequent effects on trust and behavioral intentions. The findings indicate that the presence of empathy, regardless of its type, increases the perception of warmth, leading to greater trust and intention to use. As predicted, the chatbot's perceived authenticity is diminished by compassionate and sympathetic reactions, hence negating the favorable impact shown in both tests. A third study fails to reproduce this counterproductive impact in interactions between humans. This research emphasizes that empathy is not evenly distributed in human-bot interactions. It also presents the idea of 'perceived authenticity' and shows that uniquely human characteristics can have a negative effect by seeming inauthentic while interacting with chatbots. Implementing empathy to healthcare chatbots is considered promising to create a sense of human warmth. However, existing research frequently overlooks the multidimensionality of empathy, leading to an insufficient understanding if artificial empathy is perceived similarly to interpersonal empathy. This paper argues that implementing experiential expressions of empathy may have unintended negative consequences as they might feel inauthentic. Instead, providing instrumental support could be more suitable for modeling artificial empathy as it aligns better with computer-like schemas towards chatbots. Two experimental studies using healthcare chatbots examine the effect of empathetic (feeling with), sympathetic (feeling for), and behavioral-empathetic (empathetic helping) vs. non-empathetic responses on perceived warmth, perceived authenticity, and their consequences on trust and using intentions. Results reveal that any kind of empathy (vs. no empathy) enhances perceived warmth resulting in higher trust and using intentions. As hypothesized, empathetic, and sympathetic responses reduce the chatbot's perceived authenticity suppressing this positive effect in both studies. A third study does not replicate this backfiring effect in human-human interactions. This research thus highlights that empathy does not equally apply to human-bot interactions. It further introduces the concept of ‘perceived authenticity’ and demonstrates that distinctively human attributes might backfire by feeling inauthentic in interactions with chatbots.\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Combine texts\n",
        "texts = (plagiarized_texts + ' ' + original_texts).astype(str)\n",
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gSpYva4lO5Jg"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts\n",
        "tokens = [word for sentence in texts for word in sentence.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUPwWpLDPFUb",
        "outputId": "71ea4df6-6b79-4d30-f6bf-c94f8de720ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8405"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get vocabulary size\n",
        "vocabulary_size = len(set(tokens))\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Jg7Z1C4WPSCX"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "bdbIjHNvRjXM"
      },
      "outputs": [],
      "source": [
        "# Convert texts to sequences and pad them\n",
        "sequences_plagiarized = tokenizer.texts_to_sequences(plagiarized_texts)\n",
        "sequences_original = tokenizer.texts_to_sequences(original_texts)\n",
        "padded_sequences_plagiarized = pad_sequences(sequences_plagiarized, maxlen=max_sequence_length)\n",
        "padded_sequences_original = pad_sequences(sequences_original, maxlen=max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW56IBpRUhDM",
        "outputId": "f2ab36ca-b055-4f0d-933a-7697cc35a12d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ninput_layer1 = Input(shape=(max_sequence_length,))\\ninput_layer2 = Input(shape=(max_sequence_length,))\\n\\nembedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\\n\\nlstm_layer = LSTM(units=50)\\n\\nx1 = embedding_layer(input_layer1)\\nx1 = lstm_layer(x1)\\n\\nx2 = embedding_layer(input_layer2)\\nx2 = lstm_layer(x2)\\n\\ndistance_layer = Lambda(lambda x: tf.keras.backend.abs(x[0] - x[1]),\\n                        output_shape=lambda _: (1,))([x1, x2])\\n\\noutput_layer = Dense(num_classes, activation='softmax')(distance_layer)\\n\\nmodel = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\\n\\nmodel.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\n\\ncallbacks = [\\n    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\\n]\\n\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define model (First version)\n",
        "\"\"\"\n",
        "input_layer1 = Input(shape=(max_sequence_length,))\n",
        "input_layer2 = Input(shape=(max_sequence_length,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
        "\n",
        "lstm_layer = LSTM(units=50)\n",
        "\n",
        "x1 = embedding_layer(input_layer1)\n",
        "x1 = lstm_layer(x1)\n",
        "\n",
        "x2 = embedding_layer(input_layer2)\n",
        "x2 = lstm_layer(x2)\n",
        "\n",
        "distance_layer = Lambda(lambda x: tf.keras.backend.abs(x[0] - x[1]),\n",
        "                        output_shape=lambda _: (1,))([x1, x2])\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(distance_layer)\n",
        "\n",
        "model = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 800)]                0         []                            \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)       [(None, 800)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)     (None, 800, 300)             2521500   ['input_15[0][0]',            \n",
            "                                                                     'input_16[0][0]']            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 800, 100)             160400    ['embedding_7[0][0]',         \n",
            "                                                                     'embedding_7[1][0]']         \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 50)                   30200     ['lstm[0][0]',                \n",
            "                                                                     'lstm[1][0]']                \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate  (None, 100)                  0         ['lstm_1[0][0]',              \n",
            " )                                                                   'lstm_1[1][0]']              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 128)                  12928     ['concatenate_7[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 128)                  0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 3)                    387       ['dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2725415 (10.40 MB)\n",
            "Trainable params: 2725415 (10.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define model (Second version)\n",
        "from keras.layers import LSTM, Embedding, Dense, Input, Concatenate, Dropout # type: ignore\n",
        "\n",
        "# Define model\n",
        "input_layer1 = Input(shape=(max_sequence_length,))\n",
        "input_layer2 = Input(shape=(max_sequence_length,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
        "\n",
        "# First LSTM layer\n",
        "lstm_layer1 = LSTM(units=100, return_sequences=True)\n",
        "x1 = embedding_layer(input_layer1)\n",
        "x1 = lstm_layer1(x1)\n",
        "\n",
        "# Second LSTM layer\n",
        "lstm_layer2 = LSTM(units=50)\n",
        "x1 = lstm_layer2(x1)\n",
        "\n",
        "# Repeat for the second input\n",
        "x2 = embedding_layer(input_layer2)\n",
        "x2 = lstm_layer1(x2)\n",
        "x2 = lstm_layer2(x2)\n",
        "\n",
        "# Concatenate the LSTM outputs\n",
        "concatenated = Concatenate()([x1, x2])\n",
        "\n",
        "# Add additional layers for processing\n",
        "x = Dense(128, activation='relu')(concatenated)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=[input_layer1, input_layer2], outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "]\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii6hTkO0cFBF",
        "outputId": "71c07100-ac7a-4091-bb54-b7d5ab8e7ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 800)]                0         []                            \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)       [(None, 800)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)     (None, 800, 300)             2521500   ['input_15[0][0]',            \n",
            "                                                                     'input_16[0][0]']            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 800, 100)             160400    ['embedding_7[0][0]',         \n",
            "                                                                     'embedding_7[1][0]']         \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 50)                   30200     ['lstm[0][0]',                \n",
            "                                                                     'lstm[1][0]']                \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate  (None, 100)                  0         ['lstm_1[0][0]',              \n",
            " )                                                                   'lstm_1[1][0]']              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 128)                  12928     ['concatenate_7[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 128)                  0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 3)                    387       ['dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2725415 (10.40 MB)\n",
            "Trainable params: 2725415 (10.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Show the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "17/17 [==============================] - 21s 1s/step - loss: 1.1025 - accuracy: 0.2973 - val_loss: 1.1006 - val_accuracy: 0.3030\n",
            "Epoch 2/20\n",
            "17/17 [==============================] - 19s 1s/step - loss: 1.0892 - accuracy: 0.4280 - val_loss: 1.0967 - val_accuracy: 0.3258\n",
            "Epoch 3/20\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.9442 - accuracy: 0.6155 - val_loss: 0.9609 - val_accuracy: 0.4545\n",
            "Epoch 4/20\n",
            "17/17 [==============================] - 19s 1s/step - loss: 0.5421 - accuracy: 0.7330 - val_loss: 1.0688 - val_accuracy: 0.4773\n",
            "Epoch 5/20\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.4330 - accuracy: 0.8087 - val_loss: 1.0514 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "17/17 [==============================] - 19s 1s/step - loss: 0.2762 - accuracy: 0.8788 - val_loss: 1.0747 - val_accuracy: 0.5985\n",
            "Epoch 7/20\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.1857 - accuracy: 0.9261 - val_loss: 1.1230 - val_accuracy: 0.5985\n",
            "Epoch 8/20\n",
            "17/17 [==============================] - 18s 1s/step - loss: 0.2458 - accuracy: 0.9072 - val_loss: 1.1039 - val_accuracy: 0.6364\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x305423b80>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the model\n",
        "model.fit([padded_sequences_plagiarized, padded_sequences_original], labels, epochs=20,\n",
        "          batch_size=32, validation_split=0.2, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ESjzjNQ_WXEO"
      },
      "outputs": [],
      "source": [
        "# Preprocess function\n",
        "def preprocess(sentence):\n",
        "  sequence = pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen=max_sequence_length)\n",
        "  return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QhKEarqWGgB",
        "outputId": "9c58f4f6-1c8e-484f-f4bb-ec6ef9a48499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 188ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# FID-01.txt and org-076.txt -> 0\n",
        "prediction = model.predict([\n",
        "    preprocess(\"This article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints. Traditional cheating detection methods have many disadvantages, such as difficult to detect covert equipment cheating, multi-source cheating, difficult to distinguish plagiarists from plagiarists, difficult to distinguish plagiarists from victims, or plagiarism from coincidences. To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics. In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields. In this paper, the concept of knowledge point mastery Index is introduced to measure students’ mastery of a certain knowledge point, and a test method of cheating based on improved cognitive diagnostic model is proposed. Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader. We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB). Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems. The experiments show that the precision and recall rate of this method are significantly higher than those of the method based on the false-same rate, the method based on the false-same rate and the right-same rate and the method based on the Person-Fit index.\"), \n",
        "    preprocess(\"This article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints. To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics. In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields. In this regard, we introduce a general barrier function and prescribed performance methodology to tackle constrained range impediments effectively. Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader. With the aid of the dynamic surface technology, we propose a decentralized adaptive event-triggering fault-tolerant (ETFT) formation control strategy. We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB). Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems.\")])\n",
        "predicted_classes = np.argmax(prediction, axis=1)\n",
        "predicted_classes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 82ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# FID-03.txt and Org-016.txt -> 1\n",
        "prediction = model.predict([\n",
        "    preprocess(\"At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response. However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc. At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance. The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively. This paper designed an EP Vision System (VS) based on AI technology. The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions. It showed the positive relationship between AI technology and EP VS. The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. \"), \n",
        "    preprocess(\"At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response. However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc. At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance. This paper designed an EP Vision System (VS) based on AI technology. The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions. The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent. It showed the positive relationship between AI technology and EP VS.\")])\n",
        "predicted_classes = np.argmax(prediction, axis=1)\n",
        "predicted_classes[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 84ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# FID-09.txt and Org-109.txt -> 2\n",
        "prediction = model.predict([\n",
        "    preprocess(\"Drug designing and development represent crucial areas of research for pharmaceutical companies and chemical scientists. However, challenges such as low efficacy, off-target delivery, time consumption, and high cost hinder progress in drug design and discovery. Additionally, the complexity and volume of data from genomics, proteomics, microarray data, and clinical trials pose significant obstacles in the drug discovery pipeline. Artificial intelligence (AI) and machine learning (ML) technologies have revolutionized drug discovery and development, particularly through the use of artificial neural networks and deep learning algorithms. These technologies have modernized various processes in drug discovery, including peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Historical evidence supports the implementation of AI and deep learning in drug discovery. Furthermore, novel data mining, curation, and management techniques have provided critical support to newly developed modeling algorithms. In summary, advancements in AI and deep learning offer significant opportunities for rational drug design and discovery, ultimately benefiting mankind. Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. \"), \n",
        "    preprocess(\"Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. \")])\n",
        "predicted_classes = np.argmax(prediction, axis=1)\n",
        "predicted_classes[0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
